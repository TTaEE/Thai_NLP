{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/TTaEE/Thai_NLP/blob/master/word_segmentation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "nhqjzfWZWiWs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "local_download_path = os.path.expanduser('~/BEST2010/')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "best2010 = drive.ListFile({'q': \"'1GxDYvKHO6LZjh5w_npqAfDdkx-s7IH-E' in parents\"}).GetList()\n",
        "for article in best2010:\n",
        "  article_path = os.path.join(local_download_path,article['title'])\n",
        "  try:\n",
        "    os.makedirs(article_path)\n",
        "  except: pass\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents\".format(article['id'])}).GetList()\n",
        "  for file in file_list:\n",
        "    file_path = os.path.join(article_path, file['title'])\n",
        "    f_ = drive.CreateFile({'id': file['id']})\n",
        "    f_.GetContentFile(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w404HLCMhyc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38f02f7b-5ad0-4387-abd7-a4fbc0ae4b5e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, \\\n",
        "                         Flatten, Dropout, \\\n",
        "                         GRU, Bidirectional\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GJU5cSPCiMSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CHARS_LIST = [\n",
        "    u'\\n', u' ', u'!', u'\"', u'#', u'$', u'%', u'&', \"'\", u'(', u')', u'*', u'+',\n",
        "    u',', u'-', u'.', u'/', u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8',\n",
        "    u'9', u':', u';', u'<', u'=', u'>', u'?', u'@', u'A', u'B', u'C', u'D', u'E',\n",
        "    u'F', u'G', u'H', u'I', u'J', u'K', u'L', u'M', u'N', u'O', u'P', u'Q', u'R',\n",
        "    u'S', u'T', u'U', u'V', u'W', u'X', u'Y', u'Z', u'[', u'\\\\', u']', u'^', u'_',\n",
        "    u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k', u'l', u'm',\n",
        "    u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v', u'w', u'x', u'y',\n",
        "    u'z', u'}', u'~', u'ก', u'ข', u'ฃ', u'ค', u'ฅ', u'ฆ', u'ง', u'จ', u'ฉ', u'ช',\n",
        "    u'ซ', u'ฌ', u'ญ', u'ฎ', u'ฏ', u'ฐ', u'ฑ', u'ฒ', u'ณ', u'ด', u'ต', u'ถ', u'ท',\n",
        "    u'ธ', u'น', u'บ', u'ป', u'ผ', u'ฝ', u'พ', u'ฟ', u'ภ', u'ม', u'ย', u'ร', u'ฤ',\n",
        "    u'ล', u'ว', u'ศ', u'ษ', u'ส', u'ห', u'ฬ', u'อ', u'ฮ', u'ฯ', u'ะ', u'ั', u'า',\n",
        "    u'ำ', u'ิ', u'ี', u'ึ', u'ื', u'ุ', u'ู', u'ฺ', u'เ', u'แ', u'โ', u'ใ', u'ไ',\n",
        "    u'ๅ', u'ๆ', u'็', u'่', u'้', u'๊', u'๋', u'์', u'ํ', u'๐', u'๑', u'๒', u'๓',\n",
        "    u'๔', u'๕', u'๖', u'๗', u'๘', u'๙', u'‘', u'’', u'\\ufeff', u'unk'\n",
        "]\n",
        "\n",
        "CHARS_MAP = {v: k for k, v in enumerate(CHARS_LIST)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_weScEqi0Yo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_train_set = \"BEST2010\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWbYV-YZiTzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_target_feature(text, n_pad=21):\n",
        "    pad = int((n_pad-1)/2)\n",
        "    text = re.sub('<[^>]*>', '',text)\n",
        "    text = [' ']*pad + list(text) + [' ']*pad\n",
        "    char = list()\n",
        "    tar = list()\n",
        "    for ch in text:\n",
        "        char.append(ch)\n",
        "        if ch == '|':\n",
        "            char.pop()\n",
        "            tar.pop()\n",
        "            tar.append(True)\n",
        "        else:\n",
        "            tar.append(False)     \n",
        "    return pd.DataFrame(data = {'n':char,'f':tar})\n",
        "    \n",
        "\n",
        "def build_n_gram_df(df,n_pad):\n",
        "    ng = df\n",
        "    pad = int((n_pad-1)/2)\n",
        "    for i in range(pad):\n",
        "        ng['n+{0}'.format(i+1)] = ng['n'] .shift(-i-1)\n",
        "        ng['n{0}'.format(-i-1)] = ng['n'] .shift(i+1)\n",
        "    \n",
        "    ng = ng[['n-{0}'.format(i+1) for i in range(pad)] + ['n'] + ['n+{0}'.format(i+1) for i in range(pad)] + ['f']]\n",
        "    return ng[pad:-pad]\n",
        "  \n",
        "def evaluate(x_test, y_test ,model):\n",
        "\n",
        "    y_predict = model.predict([x_test])\n",
        "    y_predict = (y_predict.ravel() > 0.5).astype(int)\n",
        "\n",
        "    f1score = f1_score(y_test, y_predict)\n",
        "    precision = precision_score(y_test, y_predict)\n",
        "    recall = recall_score(y_test, y_predict)\n",
        "\n",
        "    return f1score, precision, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xqk2zQqkibUn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bi_rnn_model(n_gram, v_size, em_size, hidden_size, dropout):\n",
        "    _input = Input(shape=(n_gram,))\n",
        "    embedding_vectors = Embedding(v_size, em_size)(_input)\n",
        "    embedding = Dropout(0.2)(embedding_vectors)\n",
        "    gru_cell = GRU(hidden_size, recurrent_dropout=dropout, dropout=dropout, return_sequences=True)\n",
        "    hidden_layer1 = Bidirectional(gru_cell)(embedding)\n",
        "    gru_cell = GRU(hidden_size, recurrent_dropout=dropout, dropout=dropout, return_sequences=True)\n",
        "    hidden_layer2 = Bidirectional(gru_cell)(hidden_layer1)\n",
        "    x = Flatten()(hidden_layer2)\n",
        "    dense = Dense(em_size, activation='relu')(x)\n",
        "    sigmoid = Dense(1, activation='sigmoid')(dense)\n",
        "    model = Model(inputs=_input, outputs=sigmoid)\n",
        "    model.compile(optimizer=Adam(),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYeiXZjAiomA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "2dafee09-6623-4a32-814d-2d816926c57c"
      },
      "cell_type": "code",
      "source": [
        "articles = ['novel','news','article','encyclopedia']\n",
        "ngram = 21\n",
        "v_size = 178\n",
        "em_size = 128\n",
        "hidden_size = 200\n",
        "droupout = 0.5\n",
        "\n",
        "model = bi_rnn_model(ngram, v_size, em_size, hidden_size, droupout)\n",
        "\n",
        "for article in articles:\n",
        "    \n",
        "    _path = os.path.join(best_train_set,article)\n",
        "    _files = os.listdir(_path)\n",
        "    print('train with {0}'.format(article))\n",
        "    \n",
        "    # load text\n",
        "    train_data = ''\n",
        "    test_data = ''\n",
        "    \n",
        "    train_files, test_files = train_test_split(_files, test_size = 0.1, random_state=None)\n",
        "    for _file in train_files:\n",
        "        with open(os.path.join(_path,_file)) as file:\n",
        "            train_data += file.read()\n",
        "    for _file in test_files:\n",
        "        with open(os.path.join(_path,_file)) as file:\n",
        "            test_data += file.read()\n",
        "    \n",
        "    # prepare data\n",
        "    train_df = build_target_feature(train_data)\n",
        "    test_df = build_target_feature(test_data)\n",
        "    \n",
        "    train_df['n'] = train_df['n'].map(lambda ch: CHARS_MAP.get(ch,178))\n",
        "    test_df['n'] = test_df['n'].map(lambda ch: CHARS_MAP.get(ch,178))\n",
        "    \n",
        "    pre_train_df = build_n_gram_df(train_df, ngram)\n",
        "    pre_test_df = build_n_gram_df(test_df, ngram)\n",
        "    \n",
        "    x_train = pre_train_df.drop(['f'], axis=1).as_matrix()\n",
        "    x_test = pre_test_df.drop(['f'], axis=1).as_matrix()\n",
        "    y_train = pre_train_df['f'].as_matrix()\n",
        "    y_test = pre_test_df['f'].as_matrix()\n",
        "    \n",
        "    # train model\n",
        "    model.fit(x_train,y_train,batch_size=4096,epochs=3)\n",
        "    # test model\n",
        "    y_predict = model.predict([x_test])\n",
        "    y_predict = (y_predict.ravel() > 0.5).astype(int)\n",
        "\n",
        "    f1score, precision, recall = evaluate(x_test, y_test ,model)\n",
        "    \n",
        "    print ('score - \\n F1 : {0} \\n Precision : {1} \\n Recall : {2}'.format(f1score, precision, recall))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train with novel\n",
            "Epoch 1/3\n",
            "3506176/5116411 [===================>..........] - ETA: 6:29 - loss: 0.1263 - acc: 0.9494"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5116411/5116411 [==============================] - 1235s 241us/step - loss: 0.1104 - acc: 0.9566\n",
            "Epoch 2/3\n",
            "1343488/5116411 [======>.......................] - ETA: 15:09 - loss: 0.0669 - acc: 0.9756"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5116411/5116411 [==============================] - 1233s 241us/step - loss: 0.0595 - acc: 0.9784\n",
            "Epoch 3/3\n",
            " 360448/5116411 [=>............................] - ETA: 19:09 - loss: 0.0483 - acc: 0.9823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5116411/5116411 [==============================] - 1234s 241us/step - loss: 0.0454 - acc: 0.9839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "score - \n",
            " F1 : 0.9744298548721493 \n",
            " Precision : 0.9658050547306237 \n",
            " Recall : 0.9832100848339717\n",
            "train with news\n",
            "Epoch 1/3\n",
            "1916928/5839801 [========>.....................] - ETA: 15:47 - loss: 0.0684 - acc: 0.9743"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5839801/5839801 [==============================] - 1410s 241us/step - loss: 0.0548 - acc: 0.9798\n",
            "Epoch 2/3\n",
            " 294912/5839801 [>.............................] - ETA: 22:24 - loss: 0.0388 - acc: 0.9858"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5839801/5839801 [==============================] - 1410s 241us/step - loss: 0.0376 - acc: 0.9865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/3\n",
            "5836800/5839801 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9885"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5839801/5839801 [==============================] - 1409s 241us/step - loss: 0.0321 - acc: 0.9885\n",
            "score - \n",
            " F1 : 0.9792475862834314 \n",
            " Precision : 0.9763839271600645 \n",
            " Recall : 0.9821280926031928\n",
            "train with article\n",
            "Epoch 1/3\n",
            "1011712/4051671 [======>.......................] - ETA: 12:12 - loss: 0.0408 - acc: 0.9859"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4051671/4051671 [==============================] - 977s 241us/step - loss: 0.0344 - acc: 0.9881\n",
            "Epoch 2/3\n",
            " 700416/4051671 [====>.........................] - ETA: 13:29 - loss: 0.0268 - acc: 0.9907"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4051671/4051671 [==============================] - 978s 241us/step - loss: 0.0263 - acc: 0.9910\n",
            "Epoch 3/3\n",
            " 557056/4051671 [===>..........................] - ETA: 14:04 - loss: 0.0226 - acc: 0.9923"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4051671/4051671 [==============================] - 978s 241us/step - loss: 0.0231 - acc: 0.9921\n",
            "score - \n",
            " F1 : 0.9843264614515672 \n",
            " Precision : 0.9802728058016432 \n",
            " Recall : 0.9884137819279305\n",
            "train with encyclopedia\n",
            "Epoch 1/3\n",
            " 299008/3955291 [=>............................] - ETA: 14:45 - loss: 0.0502 - acc: 0.9825"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3955291/3955291 [==============================] - 955s 241us/step - loss: 0.0389 - acc: 0.9865\n",
            "Epoch 2/3\n",
            " 417792/3955291 [==>...........................] - ETA: 14:15 - loss: 0.0298 - acc: 0.9898"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3955291/3955291 [==============================] - 955s 242us/step - loss: 0.0300 - acc: 0.9898\n",
            "Epoch 3/3\n",
            " 471040/3955291 [==>...........................] - ETA: 14:01 - loss: 0.0262 - acc: 0.9910"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3955291/3955291 [==============================] - 955s 241us/step - loss: 0.0263 - acc: 0.9910\n",
            "score - \n",
            " F1 : 0.9735175216723956 \n",
            " Precision : 0.9661528955408328 \n",
            " Recall : 0.9809952858549508\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}